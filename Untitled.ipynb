{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e187257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md       config.yaml     run.py          test.txt        \u001b[34mutils\u001b[m\u001b[m/\r\n",
      "Untitled.ipynb  \u001b[34mmodels\u001b[m\u001b[m/         runpol.py       train.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to preprocess the dataset and extract valid sentences\n",
    "def preprocess_dataset(dataset):\n",
    "    global L1\n",
    "    valid_sentences = []\n",
    "    current_sentence = []\n",
    "    for line in dataset:\n",
    "        L1=line\n",
    "        if line.startswith('#'):  # Ignore comments\n",
    "            continue\n",
    "        if not line.strip():  # End of sentence\n",
    "            if current_sentence:\n",
    "                valid_sentences.append(current_sentence)\n",
    "                current_sentence = []\n",
    "        else:\n",
    "            parts = line.split(' ')\n",
    "            if len(parts) >= 5:  # Ensure it's a valid line with at least 5 columns\n",
    "                word = parts[1]\n",
    "                pos_tag = parts[3]\n",
    "                head = int(parts[6]) if parts[6] != '_' else None\n",
    "                dependency_relation = parts[7] if len(parts) > 7 else None\n",
    "                current_sentence.append((word, pos_tag, head, dependency_relation))\n",
    "    return valid_sentences\n",
    "\n",
    "# Function to extract vocabulary, POS tags, and dependency relations from the dataset\n",
    "def extract_vocab_pos_dep(dataset):\n",
    "    vocabulary = set()\n",
    "    pos_tags = set()\n",
    "    dependency_relations = set()\n",
    "    for sentence in dataset:\n",
    "        for token in sentence:\n",
    "            vocabulary.add(token[0])  # Token\n",
    "            pos_tags.add(token[1])  # POS\n",
    "            dependency_relations.add(token[3])  # Dependency Relation\n",
    "    return vocabulary, pos_tags, dependency_relations\n",
    "\n",
    "# Function to generate binary feature vector for a given configuration and transition\n",
    "def generate_feature_vector(configuration, transition, vocabulary, pos_tags, dependency_relations):\n",
    "    # Extract configuration elements\n",
    "    stack, buffer, arcs = configuration\n",
    "    \n",
    "    # Initialize feature vector\n",
    "    feature_vector = np.zeros(4 * (2 * len(vocabulary) + 3 * len(pos_tags) + 4 * len(dependency_relations)))\n",
    "    \n",
    "    # Define helper function to set feature values\n",
    "    def set_feature_value(condition, value):\n",
    "        nonlocal feature_vector\n",
    "        feature_vector[condition] = value\n",
    "    \n",
    "    # Set feature values based on transition and configuration elements\n",
    "    offset = 0\n",
    "    if transition == 'LA':\n",
    "        offset = 0\n",
    "    elif transition == 'RA':\n",
    "        offset = 1\n",
    "    elif transition == 'RE':\n",
    "        offset = 2\n",
    "    elif transition == 'SH':\n",
    "        offset = 3\n",
    "    \n",
    "    # TO DO: Implement feature extraction based on configuration\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "# Function to apply transitions on the parser configuration\n",
    "def apply_transition(stack, buffer, arcs, transition):\n",
    "    if transition == 'LA' and len(stack) >= 1 and len(buffer) >= 1:\n",
    "        head, dependent = stack[-1], buffer[0]\n",
    "        arcs.append((head, dependent))\n",
    "        stack.pop()\n",
    "    elif transition == 'RA' and len(stack) >= 1 and len(buffer) >= 1:\n",
    "        head, dependent = stack[-1], buffer.pop(0)\n",
    "        arcs.append((head, dependent))\n",
    "    elif transition == 'REDUCE' and len(stack) >= 1:\n",
    "        stack.pop()\n",
    "    elif transition == 'SHIFT' and len(buffer) >= 1:\n",
    "        stack.append(buffer.pop(0))\n",
    "\n",
    "# Function to evaluate predictions using Unlabeled Attachment Score (UAS) metric\n",
    "def evaluate(predictions, gold_standard):\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    for pred_sent, gold_sent in zip(predictions, gold_standard):\n",
    "        for (gold_head, gold_dep), (pred_head, pred_dep) in zip(gold_sent, pred_sent):\n",
    "            if gold_head == pred_head:\n",
    "                total_correct += 1\n",
    "            total_tokens += 1\n",
    "    uas = total_correct / total_tokens\n",
    "    return uas\n",
    "\n",
    "# Function to read data using the provided function\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    current_sentence = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            if not line:\n",
    "                if current_sentence:\n",
    "                    data.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                parts = line.split(' ')\n",
    "                if len(parts) >= 5:  # Ensure it's a valid line with at least 5 columns\n",
    "                    word = parts[1]\n",
    "                    pos_tag = parts[3]\n",
    "                    head = int(parts[4]) if parts[4] != '_' else None\n",
    "                    dependency_relation = parts[5] if len(parts) > 5 else None\n",
    "                    current_sentence.append((word, pos_tag, head, dependency_relation))\n",
    "        if current_sentence:  # Append the last sentence if not empty\n",
    "            data.append(current_sentence)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Read train and test data\n",
    "train_data = read_data('train.txt')\n",
    "test_data = read_data('test.txt')\n",
    "\n",
    "# Preprocess train and test data\n",
    "#train_data = preprocess_dataset(train_data)\n",
    "#test_data = preprocess_dataset(test_data)\n",
    "\n",
    "# Extract vocabulary, POS tags, and dependency relations\n",
    "vocabulary, pos_tags, dependency_relations = extract_vocab_pos_dep(train_data)\n",
    "\n",
    "# Generate feature vectors for training instances\n",
    "train_instances = []\n",
    "for sentence in train_data:\n",
    "    stack = []\n",
    "    buffer = sentence[:]\n",
    "    arcs = []\n",
    "    while len(buffer) > 0:\n",
    "        for transition in ['LA', 'RA', 'RE', 'SH']:\n",
    "            feature_vector = generate_feature_vector((stack, buffer, arcs), transition, vocabulary, pos_tags, dependency_relations)\n",
    "            train_instances.append((feature_vector, transition))\n",
    "            apply_transition(stack, buffer, arcs, transition)\n",
    "print(\"-----\",train_instances)\n",
    "# Initialize classifier weights\n",
    "weights = np.random.rand(len(train_instances[0][0]))\n",
    "\n",
    "# Train classifier using online learning\n",
    "learning_rate = 0.1\n",
    "for feature_vector, transition in train_instances:\n",
    "    score = np.dot(weights, feature_vector)\n",
    "    gold_standard = 1 if transition == gold_transition else 0\n",
    "    weights += learning_rate * (gold_standard - score) * feature_vector\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = []\n",
    "for sentence in test_data:\n",
    "    stack = []\n",
    "    buffer = sentence[:]\n",
    "    arcs = []\n",
    "    while len(buffer) > 0:\n",
    "        for transition in ['LA', 'RA', 'RE', 'SH']:\n",
    "            feature_vector = generate_feature_vector((stack, buffer, arcs), transition, vocabulary, pos_tags, dependency_relations)\n",
    "            score = np.dot(weights, feature_vector)\n",
    "            predicted_transition = 'SH'  # Default transition in case of invalid predictions\n",
    "            if transition == 'LA' and len(stack) >= 2:\n",
    "                predicted_transition = 'LA'\n",
    "            elif transition == 'RA' and len(stack) >= 2:\n",
    "                predicted_transition = 'RA'\n",
    "            elif transition == 'RE' and len(stack) >= 1:\n",
    "                predicted_transition = 'RE'\n",
    "            elif transition == 'SH':\n",
    "                predicted_transition = 'SH'\n",
    "            apply_transition(stack, buffer, arcs, predicted_transition)\n",
    "    test_predictions.append(arcs)\n",
    "\n",
    "# Evaluate predictions using UAS metric\n",
    "uas = evaluate(test_predictions, gold_standard)\n",
    "print(\"Unlabeled Attachment Score (UAS):\", uas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c25fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
